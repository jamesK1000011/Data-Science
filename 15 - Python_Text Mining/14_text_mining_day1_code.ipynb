{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## 14 TEXT MINING DAY1 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Directory settings  ####\n",
    "\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Linux).\n",
    "main_dir = \"/home/[username]/Desktop/af-werx\"\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Mac).\n",
    "main_dir = '/Users/[username]/Desktop/af-werx'\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Windows).\n",
    "main_dir = \"C:\\\\Users\\\\[username]\\\\Desktop\\\\af-werx\"\n",
    "# Make `data_dir` from the `main_dir` and\n",
    "# remainder of the path to data directory (for Mac).\n",
    "data_dir = main_dir + \"/data\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and\n",
    "# remainder of the path to data directory (for Windows).\n",
    "data_dir = main_dir + \"\\\\data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages with tools for text processing.\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Working directory  ####\n",
    "\n",
    "# Set working directory.\n",
    "os.chdir(data_dir)\n",
    "# Check working directory.\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 25: Loading text data  ####\n",
    "\n",
    "# Load the corpus.\n",
    "NYT = pd.read_csv(data_dir + '/NYT_article_data.csv')\n",
    "\n",
    "print(NYT.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Look at the first few columns  ####\n",
    "\n",
    "# Look at the columns.\n",
    "print(NYT.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 27: Creating a list of snippets  ####\n",
    "\n",
    "# Isolate the snippet column.\n",
    "NYT_snippet = NYT[\"snippet\"]\n",
    "# Look at a sample of the snippets column, 0-20.\n",
    "print(NYT[\"snippet\"][0:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 29: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Tokenization: split each snippet into words  ####\n",
    "\n",
    "# Tokenize each snippet into a large list of tokenized snippets.\n",
    "NYT_tokenized = [word_tokenize(NYT_snippet[i]) for i in range(0,len(NYT_snippet))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 39: Save the first tokenized snippet  ####\n",
    "\n",
    "# Let's take a look at the first tokenized snippet.\n",
    "snippet_words = NYT_tokenized[0]\n",
    "print(snippet_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 41: Convert characters to lower case  ####\n",
    "\n",
    "# 1. Convert to lower case.\n",
    "snippet_words = [word.lower() for word in snippet_words]\n",
    "print(snippet_words[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 43: Remove stop words  ####\n",
    "\n",
    "# 2. Remove stopwords.\n",
    "# Get common English stop words.\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words[:10])\n",
    "\n",
    "# Remove stop words.\n",
    "snippet_words = [word for word in snippet_words if not word in stop_words]\n",
    "print(snippet_words[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 45: Remove non-alphabetical characters  ####\n",
    "\n",
    "# 3. Remove punctuation and any non-alphabetical characters.\n",
    "snippet_words = [word for word in snippet_words if word.isalpha()]\n",
    "print(snippet_words[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 48: Stem words  ####\n",
    "\n",
    "# 4. Stem words.\n",
    "snippet_words = [PorterStemmer().stem(word) for word in snippet_words]\n",
    "print(snippet_words[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 49: Implementing pre-processing steps on a corpus  ####\n",
    "\n",
    "# Create a list for clean snippets.\n",
    "NYT_clean = [None] * len(NYT_tokenized)\n",
    "# Create a list of word counts for each clean snippet.\n",
    "word_counts_per_snippet = [None] * len(NYT_tokenized)\n",
    "\n",
    "# Process words in all snippets.\n",
    "for i in range(len(NYT_tokenized)):\n",
    "    # 1. Convert to lower case.\n",
    "    NYT_clean[i] = [snippet.lower() for snippet in NYT_tokenized[i]]\n",
    "\n",
    "    # 2. Remove stopwords.\n",
    "    NYT_clean[i] = [word for word in NYT_clean[i] if not word in stop_words]\n",
    "\n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    NYT_clean[i] = [word for word in NYT_clean[i] if word.isalpha()]\n",
    "\n",
    "    # 4. Stem words.\n",
    "    NYT_clean[i] = [PorterStemmer().stem(word) for word in NYT_clean[i]]\n",
    "\n",
    "    # Record the word count per snippet.\n",
    "    word_counts_per_snippet[i] = len(NYT_clean[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 50: Inspect results  ####\n",
    "\n",
    "print(NYT_clean[0][:10])\n",
    "print(NYT_clean[5][:10])\n",
    "print(NYT_clean[10][:10])\n",
    "print(NYT_clean[15][:10])\n",
    "print(NYT_clean[20][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 51: Removing empty and very short snippets  ####\n",
    "\n",
    "# Let's take a look at total word counts per snippet (for the first 10).\n",
    "print(word_counts_per_snippet[:10])\n",
    "# Plot a histogram for word counts per snippet, set bins to num of unique values in the list.\n",
    "plt.hist(word_counts_per_snippet, bins = len(set(word_counts_per_snippet)))\n",
    "\n",
    "plt.xlabel('Number of words per snippet')\n",
    "plt.ylabel('Frequency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 52: Removing empty and very short snippets (cont'd)  ####\n",
    "\n",
    "# Convert word counts list and snippets list to numpy arrays.\n",
    "word_counts_array = np.array(word_counts_per_snippet)\n",
    "NYT_array = np.array(NYT_clean)\n",
    "print(len(NYT_array))\n",
    "# Find indices of all snippets where there are greater than or equal to 5 words.\n",
    "valid_snippets = np.where(word_counts_array >= 5)[0]\n",
    "print(len(valid_snippets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 53: Removing empty and very short snippets (cont'd)  ####\n",
    "\n",
    "# Subset the NYT_array to keep only those where there are at least 5 words.\n",
    "NYT_array = NYT_array[valid_snippets]\n",
    "print(len(NYT_array))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 54: Removing empty and very short snippets (cont'd)  ####\n",
    "\n",
    "# Convert the array back to a list.\n",
    "NYT_clean = NYT_array.tolist()\n",
    "print(NYT_clean[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 55: .join() function  ####\n",
    "\n",
    "# Here is a simple example of the `.join()` function in action!\n",
    "numList = ['1', '2', '3', '4']\n",
    "print(', '.join(numList))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 56: Save processed text to file using .join()  ####\n",
    "\n",
    "# Join words in each snippet into a single character string.\n",
    "NYT_clean_list = [' '.join(snippet) for snippet in NYT_clean]\n",
    "print(NYT_clean_list[:5])\n",
    "\n",
    "# Save output file name to a variable.\n",
    "out_filename = data_dir + \"/clean_NYT.txt\"\n",
    "\n",
    "# Create a function that takes a list of character strings\n",
    "# and a name of an output file and writes it into a txt file.\n",
    "def write_lines(lines, filename):   #<- given lines to write and filename\n",
    "    joined_lines = '\\n'.join(lines) #<- join lines with line breaks\n",
    "    file = open(out_filename, 'w')  #<- open write only file\n",
    "    file.write(joined_lines)        #<- write lines to file\n",
    "    file.close()                    #<- close connection\n",
    "\n",
    "# Write sequences to file.\n",
    "write_lines(NYT_clean_list, out_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 59: Exercise 2  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 64: Create a DTM  ####\n",
    "\n",
    "# Initialize `CountVectorizer`.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Transform the list of snippets into DTM.\n",
    "X = vec.fit_transform(NYT_clean_list)\n",
    "print(X.toarray()) #<- show output as a matrix\n",
    "\n",
    "print(vec.get_feature_names()[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 65: Create a DTM (cont'd)  ####\n",
    "\n",
    "# Convert the matrix into a pandas dataframe for easier manipulation.\n",
    "DTM = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "print(DTM.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 66: DTM to dictionary of total word counts  ####\n",
    "\n",
    "# Create a convenience function that sorts and looks at first n-entries in the dictionary.\n",
    "def HeadDict(dict_x, n):\n",
    "    # Get items from the dictionary and sort them by\n",
    "    # value key in descending (i.e. reverse) order\n",
    "    sorted_x = sorted(dict_x.items(),\n",
    "    reverse = True,\n",
    "    key = lambda kv: kv[1])\n",
    "\n",
    "    # Convert sorted dictionary to a list.\n",
    "    dict_x_list = list(sorted_x)\n",
    "\n",
    "    # Return the first `n` values from the dictionary only.\n",
    "    return(dict(dict_x_list[:n]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 67: DTM to dictionary of total word counts (cont'd)  ####\n",
    "\n",
    "# Sum frequencies of each word in all documents.\n",
    "DTM.sum(axis = 0).head()\n",
    "\n",
    "# Save series as a dictionary.\n",
    "corpus_freq_dist = DTM.sum(axis = 0).to_dict()\n",
    "\n",
    "# Glance at the frequencies.\n",
    "print(HeadDict(corpus_freq_dist, 6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 69: Plot distribution of words in snippet corpus  ####\n",
    "\n",
    "# Save as a FreqDist object native to nltk.\n",
    "corpus_freq_dist = nltk.FreqDist(corpus_freq_dist)\n",
    "# Plot distribution for the entire corpus.\n",
    "plt.figure(figsize = (16, 7))\n",
    "corpus_freq_dist.plot(80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 70: Visualizing word counts with word clouds  ####\n",
    "\n",
    "# Construct a word cloud from corpus.\n",
    "wordcloud = WordCloud(max_font_size = 40, background_color = \"white\")\n",
    "wordcloud = wordcloud.generate(' '.join(NYT_clean_list))\n",
    "\n",
    "# Plot the cloud using matplotlib.\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 72: Save results as a pickle  ####\n",
    "\n",
    "pickle.dump(DTM, open('DTM.sav', 'wb'))\n",
    "pickle.dump(X, open('DTM_matrix.sav', 'wb'))\n",
    "pickle.dump(NYT_clean, open('NYT_clean.sav', 'wb'))\n",
    "pickle.dump(NYT_clean_list, open('NYT_clean_list.sav', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
