{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - Day 1 - Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Load libraries that are used in this module.\n",
    "##### Set working directory to folder where the dataset is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Set `main_dir` to the location of your `af-werx` folder (for Mac/Linux).\n",
    "##### Make `data_dir` from the `main_dir` and concatenate the remainder of the path to data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "#####  Set the working directory to `data_dir`.\n",
    "#####  Check if the working directory is updated to `data_dir'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "#####  Load the corpus from `UN_agreement_titles.csv` into a new variable `agreements` .\n",
    "#####  Print 200th agreement title and check the output to see if data is loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Make a series from the dataframe that contains only the titles and name it `titles`, then print the first 5 titles. \n",
    "##### Check the length of the series 'titles'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Tokenize each title in the series `titles` and assign it to `titles_tokenized`.\n",
    "##### Assign the first tokenized titles to `title_words` and print this out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Clean the `titles_words` in the following order:\n",
    "##### 1. Convert all characters to lower case and assign it to `titles_words`\n",
    "##### 2. Remove stop words from `titles_words` and assign it to `titles_words`\n",
    "##### 3. Remove punctuation, numbers, and all other symbols that are not letters of the alphabet \n",
    "#####    from `titles_words` and assign it to `titles_words`\n",
    "##### 4. Stem words in `titles_words` and assign it to `titles_words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Create an empty list `titles_clean` whose length is same as `titles_tokenized'.\n",
    "##### Perform the above steps on the list `titles_tokenized` and also record the length of each title in 'word_counts_per_titles'.\n",
    "##### Check the first 5 words in 300th title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### Plot a histogram for word counts per message, set bins to number of unique values in the list.\n",
    "##### Remove all the titles whose length is less than 3 words and update.\n",
    "##### Combine word tokens in each titles into a single string and save the result as a list called `titles_clean_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Use the function that takes a list of character strings\n",
    "##### and a name of an output file and writes it into a txt file we defined in class.\n",
    "```\n",
    "def write_lines(lines, filename):   #<- given lines to write and filename\n",
    "    joined_lines = '\\n'.join(lines) #<- join lines with line breaks\n",
    "    file = open(out_filename, 'w')  #<- open write only file \n",
    "    file.write(joined_lines)        #<- write lines to file\n",
    "    file.close()                    #<- close connection\n",
    "```    \n",
    "##### Save all the processed text to file name `clean_titles.txt` using the above function.\n",
    "##### Save output file name to a variable `out_filename` and call the text file \"clean_titles.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Create a DTM of the `titles_clean_list` and name it 'exercise_DTM'.\n",
    "##### Inspect `exercise_DTM`.\n",
    "##### Convert `exercise_DTM` to a pandas dataframe and print the top 5 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Use the convenience function that sorts and looks at first n-entries in the dictionary we defined in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeadDict(dict_x, n):\n",
    "    # Get items from the dictionary and sort them by\n",
    "    # value key in descending (i.e. reverse) order.\n",
    "    sorted_x = sorted(dict_x.items(),\n",
    "    reverse = True,\n",
    "    key = lambda kv: kv[1])\n",
    "    # Convert sorted dictionary to a list.\n",
    "    dict_x_list = list(sorted_x)\n",
    "    # Return the first `n` values from the dictionary only.\n",
    "    return(dict(dict_x_list[:n]))\n",
    "\n",
    "# Sum the counts of each word in all documents and save the series as a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Visualize the distribution of words in titles corpus based on their frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### Create the word cloud of the entire corpus and plot it; set `figsize` to (14, 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Save `exercise_DTM`, `titles_clean` and `titles_clean_list` files as pickles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
