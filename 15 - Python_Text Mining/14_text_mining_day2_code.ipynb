{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## 14 TEXT MINING DAY2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Directory settings  ####\n",
    "\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Linux).\n",
    "main_dir = \"/home/[username]/Desktop/af-werx\"\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Mac).\n",
    "main_dir = '/Users/[username]/Desktop/af-werx'\n",
    "# Set `main_dir` to the location of your `af-werx` folder (for Windows).\n",
    "main_dir = \"C:\\\\Users\\\\[username]\\\\Desktop\\\\af-werx\"\n",
    "# Make `data_dir` from the `main_dir` and\n",
    "# remainder of the path to data directory (for Mac).\n",
    "data_dir = main_dir + \"/data\"\n",
    "\n",
    "# Make `data_dir` from the `main_dir` and\n",
    "# remainder of the path to data directory (for Windows).\n",
    "data_dir = main_dir + \"\\\\data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `main_dir` to the location of your `af-werx` folder (for Mac).\n",
    "main_dir = '/Users/kiru/Desktop/af-werx'\n",
    "\n",
    "data_dir = main_dir + \"/data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "\n",
    "# Packages for working with text data.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Packages for getting data ready and building a LDA model.\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kiru/Desktop/af-werx/data\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Working directory  ####\n",
    "\n",
    "# Set working directory.\n",
    "os.chdir(data_dir)\n",
    "# Check working directory.\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Load objects from last class  ####\n",
    "\n",
    "processed_docs = pickle.load(open(\"NYT_clean.sav\",\"rb\"))  #<- the processed NYT snippets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 africa\n",
      "1 attack\n",
      "2 back\n",
      "3 batsmen\n",
      "4 claw\n",
      "5 find\n",
      "6 handl\n",
      "7 like\n",
      "8 live\n",
      "9 must\n",
      "10 newland\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Create a dictionary of counts   ####\n",
    "\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# The loop below iterates through the first 10 items of the dictionary and prints out the key and value.\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1)]\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 27: Document to bag-of-words dictionary  ####\n",
    "\n",
    "# We use a list comprehension to transform each doc within our processed_docs object.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# Let's look at the 1st document.\n",
    "print(bow_corpus[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"africa\") appears 1 time.\n",
      "Word 1 (\"attack\") appears 1 time.\n",
      "Word 2 (\"back\") appears 1 time.\n",
      "Word 3 (\"batsmen\") appears 1 time.\n",
      "Word 4 (\"claw\") appears 1 time.\n",
      "Word 5 (\"find\") appears 1 time.\n",
      "Word 6 (\"handl\") appears 1 time.\n",
      "Word 7 (\"like\") appears 1 time.\n",
      "Word 8 (\"live\") appears 1 time.\n",
      "Word 9 (\"must\") appears 1 time.\n",
      "Word 10 (\"newland\") appears 1 time.\n",
      "Word 11 (\"pace\") appears 1 time.\n",
      "Word 12 (\"pakistan\") appears 1 time.\n",
      "Word 13 (\"potent\") appears 1 time.\n",
      "Word 14 (\"second\") appears 1 time.\n",
      "Word 15 (\"seri\") appears 1 time.\n",
      "Word 16 (\"south\") appears 1 time.\n",
      "Word 17 (\"start\") appears 1 time.\n",
      "Word 18 (\"struggl\") appears 1 time.\n",
      "Word 19 (\"test\") appears 1 time.\n",
      "Word 20 (\"thursday\") appears 1 time.\n",
      "Word 21 (\"way\") appears 2 time.\n",
      "Word 22 (\"wicket\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 28: Document to bag-of-words  ####\n",
    "\n",
    "# Isolate the first document.\n",
    "bow_doc_1 = bow_corpus[0]\n",
    "\n",
    "# Iterate through each dictionary item using the index.\n",
    "# Print out each actual word and how many times it appears.\n",
    "for i in range(len(bow_doc_1)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], dictionary[bow_doc_1[i][0]], bow_doc_1[i][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.20123812213894005),\n",
      " (1, 0.16169735155283377),\n",
      " (2, 0.1814677368458869),\n",
      " (3, 0.23503581903178308),\n",
      " (4, 0.23503581903178308),\n",
      " (5, 0.19035769396664654),\n",
      " (6, 0.19035769396664654),\n",
      " (7, 0.13068668575528808),\n",
      " (8, 0.20123812213894005),\n",
      " (9, 0.23503581903178308),\n",
      " (10, 0.23503581903178308),\n",
      " (11, 0.21526543373872994),\n",
      " (12, 0.20123812213894005),\n",
      " (13, 0.23503581903178308),\n",
      " (14, 0.20123812213894005),\n",
      " (15, 0.20123812213894005),\n",
      " (16, 0.1814677368458869),\n",
      " (17, 0.16744042524609698),\n",
      " (18, 0.20123812213894005),\n",
      " (19, 0.16169735155283377),\n",
      " (20, 0.14376717596065594),\n",
      " (21, 0.3629354736917738),\n",
      " (22, 0.21526543373872994)]\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 30: Transform counts with TfidfModel  ####\n",
    "\n",
    "# This is the transformation.\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Apply the transformation to the entire corpus.\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "# Preview TF-IDF scores for the first document.\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 33: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=1925, num_topics=5, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 53: Running LdaMulticore  ####\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 5, id2word = dictionary, workers = 4, passes = 2)\n",
    "print(lda_model_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"suspect\" + 0.002*\"year\" + 0.002*\"make\" + 0.002*\"local\" + 0.002*\"first\" + 0.002*\"leagu\" + 0.002*\"head\" + 0.002*\"time\" + 0.002*\"presid\" + 0.002*\"new\"\n",
      "Topic: 1 Word: 0.003*\"said\" + 0.002*\"best\" + 0.002*\"tuesday\" + 0.002*\"would\" + 0.002*\"brief\" + 0.002*\"argument\" + 0.002*\"reach\" + 0.002*\"never\" + 0.002*\"hous\" + 0.002*\"secretari\"\n",
      "Topic: 2 Word: 0.002*\"new\" + 0.002*\"week\" + 0.002*\"time\" + 0.002*\"york\" + 0.002*\"profession\" + 0.002*\"murder\" + 0.002*\"offici\" + 0.002*\"championship\" + 0.002*\"appear\" + 0.002*\"final\"\n",
      "Topic: 3 Word: 0.003*\"local\" + 0.002*\"time\" + 0.002*\"govern\" + 0.002*\"latest\" + 0.002*\"new\" + 0.002*\"say\" + 0.002*\"world\" + 0.002*\"eve\" + 0.002*\"show\" + 0.002*\"order\"\n",
      "Topic: 4 Word: 0.003*\"presid\" + 0.002*\"say\" + 0.002*\"donald\" + 0.002*\"trump\" + 0.002*\"border\" + 0.002*\"kill\" + 0.002*\"said\" + 0.002*\"secur\" + 0.002*\"even\" + 0.002*\"hous\"\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 54: LDA output  ####\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pakistan', 'struggl', 'batsmen', 'must', 'find', 'way', 'handl', 'south', 'africa', 'potent', 'pace', 'attack', 'claw', 'way', 'back', 'seri', 'second', 'test', 'start', 'like', 'live', 'newland', 'wicket', 'thursday']\n",
      "\n",
      "Score: 0.967782735824585\t \n",
      "Topic: 0.002*\"new\" + 0.002*\"week\" + 0.002*\"time\" + 0.002*\"york\" + 0.002*\"profession\" + 0.002*\"murder\" + 0.002*\"offici\" + 0.002*\"championship\" + 0.002*\"appear\" + 0.002*\"final\"\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 55: Classify our documents within topics  ####\n",
    "\n",
    "# Now, we can check where each of our documents would be classified.\n",
    "# Let's look at our first document as an example:\n",
    "\n",
    "print(processed_docs[0])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 57: Exercise 2  ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4937424088619259\n"
     ]
    }
   ],
   "source": [
    "#=================================================-\n",
    "#### Slide 61: Topic coherence quick overview  ####\n",
    "\n",
    "# Compute Coherence Score using c_v.\n",
    "coherence_model_lda = CoherenceModel(model = lda_model_tfidf, texts = processed_docs, dictionary = dictionary, coherence = 'c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 63: Convenience function  ####\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start = 2, step = 3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.LdaMulticore(corpus = corpus, id2word = dictionary, num_topics = num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 64: Run compute_coherence_values function  ####\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary = dictionary, corpus = bow_corpus, texts = processed_docs, start = 2, limit = 40, step = 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 67: Visualize topics generated with LDA  ####\n",
    "\n",
    "# Prepare LDA vis object by providing:\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_tfidf,   #<- model object\n",
    "                              corpus_tfidf,#<- corpus object\n",
    "                              dictionary)  #<- dictionary object\n",
    "\n",
    "# The function takes `vis` object that we prepared above as the main argument.\n",
    "pyLDAvis.display(vis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 76: Save LDA visualization to HTML file  ####\n",
    "\n",
    "# Save the plot as a self-contained HTML file.\n",
    "pyLDAvis.save_html(vis, plot_dir+\"/NYT_LDAvis.html\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
