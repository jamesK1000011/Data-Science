{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification Day 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Import the required packages.\n",
    "##### Set the working directory to data directory.\n",
    "##### Print the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `main_dir` to the location of your `af-werx` folder (for Mac).\n",
    "main_dir = '/Users/datasociety/Desktop/af-werx'\n",
    "data_dir = main_dir + \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# Scikit-learn package for logistic regression.\n",
    "from sklearn import linear_model\n",
    "# Model set up and tuning packages from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Scikit-learn packages for evaluating model performance.\n",
    "from sklearn import metrics\n",
    "# Scikit-learn package for data preprocessing.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/datasociety/Desktop/af-werx/data\n"
     ]
    }
   ],
   "source": [
    "# Set working directory.\n",
    "os.chdir(data_dir)\n",
    "# Check working directory.\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Load the pickled dataset `ex_classification_cleaned.sav` and save it to `ex_classification`.\n",
    "##### Print the first few rows of `ex_classification`.\n",
    "##### We don't need to do additional data preparation because we've done it during last session's exercise. \n",
    "##### Check the data types for all of the variables just in case.\n",
    "##### Look at the count of each class in `income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent_house_crowded          float64\n",
      "percent_house_below_poverty    float64\n",
      "percent_16_unemployed          float64\n",
      "percent_25_without_diploma     float64\n",
      "percent_dependent              float64\n",
      "per_capita_income                int64\n",
      "hardship_index                 float64\n",
      "income                            bool\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_house_crowded</th>\n",
       "      <th>percent_house_below_poverty</th>\n",
       "      <th>percent_16_unemployed</th>\n",
       "      <th>percent_25_without_diploma</th>\n",
       "      <th>percent_dependent</th>\n",
       "      <th>per_capita_income</th>\n",
       "      <th>hardship_index</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>18.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>23939</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>17.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>38.5</td>\n",
       "      <td>23040</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.8</td>\n",
       "      <td>22.2</td>\n",
       "      <td>35787</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4</td>\n",
       "      <td>10.9</td>\n",
       "      <td>8.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>25.5</td>\n",
       "      <td>37524</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>26.2</td>\n",
       "      <td>57123</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60058</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8</td>\n",
       "      <td>12.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>21.5</td>\n",
       "      <td>71551</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.9</td>\n",
       "      <td>12.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>22.6</td>\n",
       "      <td>88669</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>35.3</td>\n",
       "      <td>40959</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>32875</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.7</td>\n",
       "      <td>8.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>35.5</td>\n",
       "      <td>27751</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>40.5</td>\n",
       "      <td>44164</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.9</td>\n",
       "      <td>13.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>14.4</td>\n",
       "      <td>39.0</td>\n",
       "      <td>26576</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.3</td>\n",
       "      <td>19.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>32.0</td>\n",
       "      <td>21323</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.1</td>\n",
       "      <td>11.6</td>\n",
       "      <td>12.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24336</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.3</td>\n",
       "      <td>13.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>27249</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.6</td>\n",
       "      <td>26282</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.1</td>\n",
       "      <td>15.3</td>\n",
       "      <td>13.8</td>\n",
       "      <td>23.5</td>\n",
       "      <td>38.6</td>\n",
       "      <td>22014</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.8</td>\n",
       "      <td>18.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>37.3</td>\n",
       "      <td>37.3</td>\n",
       "      <td>15461</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.9</td>\n",
       "      <td>20.5</td>\n",
       "      <td>13.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>36.4</td>\n",
       "      <td>15089</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>24.7</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20039</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.2</td>\n",
       "      <td>16.8</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>26.2</td>\n",
       "      <td>31908</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.8</td>\n",
       "      <td>33.9</td>\n",
       "      <td>17.3</td>\n",
       "      <td>35.4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13781</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.3</td>\n",
       "      <td>14.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>12.9</td>\n",
       "      <td>21.7</td>\n",
       "      <td>43198</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.3</td>\n",
       "      <td>28.6</td>\n",
       "      <td>22.6</td>\n",
       "      <td>24.4</td>\n",
       "      <td>37.9</td>\n",
       "      <td>15957</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.4</td>\n",
       "      <td>41.7</td>\n",
       "      <td>25.8</td>\n",
       "      <td>24.5</td>\n",
       "      <td>43.6</td>\n",
       "      <td>10934</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.2</td>\n",
       "      <td>42.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>21.3</td>\n",
       "      <td>43.2</td>\n",
       "      <td>12961</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.8</td>\n",
       "      <td>20.6</td>\n",
       "      <td>10.7</td>\n",
       "      <td>9.6</td>\n",
       "      <td>22.2</td>\n",
       "      <td>44689</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7.4</td>\n",
       "      <td>43.1</td>\n",
       "      <td>21.2</td>\n",
       "      <td>27.6</td>\n",
       "      <td>42.7</td>\n",
       "      <td>12034</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.2</td>\n",
       "      <td>30.7</td>\n",
       "      <td>15.8</td>\n",
       "      <td>54.8</td>\n",
       "      <td>33.8</td>\n",
       "      <td>10402</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.5</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.3</td>\n",
       "      <td>16.9</td>\n",
       "      <td>41.2</td>\n",
       "      <td>17949</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.5</td>\n",
       "      <td>21.6</td>\n",
       "      <td>22.8</td>\n",
       "      <td>13.1</td>\n",
       "      <td>38.6</td>\n",
       "      <td>20588</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.0</td>\n",
       "      <td>29.2</td>\n",
       "      <td>16.3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>14685</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>6.8</td>\n",
       "      <td>19.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>31.9</td>\n",
       "      <td>42.8</td>\n",
       "      <td>17104</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3.3</td>\n",
       "      <td>25.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>42.1</td>\n",
       "      <td>16563</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.8</td>\n",
       "      <td>56.5</td>\n",
       "      <td>34.6</td>\n",
       "      <td>27.5</td>\n",
       "      <td>51.5</td>\n",
       "      <td>8201</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3.3</td>\n",
       "      <td>17.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>19.2</td>\n",
       "      <td>42.9</td>\n",
       "      <td>22677</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>11.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>38.1</td>\n",
       "      <td>26353</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>8.5</td>\n",
       "      <td>14.1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>35.9</td>\n",
       "      <td>39.2</td>\n",
       "      <td>16134</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14.4</td>\n",
       "      <td>23.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>45.1</td>\n",
       "      <td>39.3</td>\n",
       "      <td>13089</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>7.2</td>\n",
       "      <td>18.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>32.9</td>\n",
       "      <td>35.6</td>\n",
       "      <td>16954</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>13.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>31.3</td>\n",
       "      <td>22694</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>11.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>41.5</td>\n",
       "      <td>38.9</td>\n",
       "      <td>12765</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>11.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>16.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>15754</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>15.8</td>\n",
       "      <td>23.4</td>\n",
       "      <td>18.2</td>\n",
       "      <td>51.5</td>\n",
       "      <td>38.8</td>\n",
       "      <td>12171</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>18.8</td>\n",
       "      <td>37.6</td>\n",
       "      <td>25113</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9.6</td>\n",
       "      <td>33.6</td>\n",
       "      <td>39.6</td>\n",
       "      <td>16907</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7.6</td>\n",
       "      <td>27.9</td>\n",
       "      <td>17.1</td>\n",
       "      <td>31.2</td>\n",
       "      <td>40.6</td>\n",
       "      <td>13231</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.8</td>\n",
       "      <td>34.4</td>\n",
       "      <td>35.9</td>\n",
       "      <td>26.3</td>\n",
       "      <td>40.7</td>\n",
       "      <td>11317</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.8</td>\n",
       "      <td>46.6</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>42.5</td>\n",
       "      <td>11888</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3.6</td>\n",
       "      <td>29.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>17285</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>36.9</td>\n",
       "      <td>23482</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4.0</td>\n",
       "      <td>27.6</td>\n",
       "      <td>28.3</td>\n",
       "      <td>18.5</td>\n",
       "      <td>41.9</td>\n",
       "      <td>15528</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>40.5</td>\n",
       "      <td>39523</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>13.7</td>\n",
       "      <td>42.6</td>\n",
       "      <td>19713</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>36.8</td>\n",
       "      <td>34381</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.8</td>\n",
       "      <td>13.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>40.3</td>\n",
       "      <td>27149</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>3.6</td>\n",
       "      <td>15.4</td>\n",
       "      <td>7.1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>30.3</td>\n",
       "      <td>25828</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>23.8</td>\n",
       "      <td>33385</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4.7</td>\n",
       "      <td>19.7</td>\n",
       "      <td>12.9</td>\n",
       "      <td>19.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>28202</td>\n",
       "      <td>49.506494</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    percent_house_crowded  percent_house_below_poverty  percent_16_unemployed  \\\n",
       "0                     7.7                         23.6                    8.7   \n",
       "1                     7.8                         17.2                    8.8   \n",
       "2                     3.8                         24.0                    8.9   \n",
       "3                     3.4                         10.9                    8.2   \n",
       "4                     0.3                          7.5                    5.2   \n",
       "5                     1.1                         11.4                    4.7   \n",
       "6                     0.8                         12.3                    5.1   \n",
       "7                     1.9                         12.9                    7.0   \n",
       "8                     1.1                          3.3                    6.5   \n",
       "9                     2.0                          5.4                    9.0   \n",
       "10                    2.7                          8.6                   12.4   \n",
       "11                    1.1                          7.5                    6.8   \n",
       "12                    3.9                         13.2                    9.9   \n",
       "13                   11.3                         19.2                   10.0   \n",
       "14                    4.1                         11.6                   12.6   \n",
       "15                    6.3                         13.1                   10.0   \n",
       "16                    5.2                         10.6                   10.0   \n",
       "17                    8.1                         15.3                   13.8   \n",
       "18                   10.8                         18.7                   14.6   \n",
       "19                    6.9                         20.5                   13.1   \n",
       "20                    6.0                         15.3                    9.2   \n",
       "21                    3.2                         16.8                    8.2   \n",
       "22                   14.8                         33.9                   17.3   \n",
       "23                    2.3                         14.7                    6.6   \n",
       "24                    6.3                         28.6                   22.6   \n",
       "25                    9.4                         41.7                   25.8   \n",
       "26                    8.2                         42.4                   19.6   \n",
       "27                    3.8                         20.6                   10.7   \n",
       "28                    7.4                         43.1                   21.2   \n",
       "29                   15.2                         30.7                   15.8   \n",
       "..                    ...                          ...                    ...   \n",
       "48                    2.5                         19.8                   20.3   \n",
       "49                    1.5                         21.6                   22.8   \n",
       "50                    4.0                         29.2                   16.3   \n",
       "51                    6.8                         19.2                   12.1   \n",
       "52                    3.3                         25.9                   19.4   \n",
       "53                    5.8                         56.5                   34.6   \n",
       "54                    3.3                         17.1                    9.6   \n",
       "55                    2.6                          8.8                   11.3   \n",
       "56                    8.5                         14.1                   16.5   \n",
       "57                   14.4                         23.6                   13.9   \n",
       "58                    7.2                         18.7                   13.4   \n",
       "59                    4.5                         18.9                   13.7   \n",
       "60                   11.9                         29.0                   23.0   \n",
       "61                   11.1                         15.6                   16.7   \n",
       "62                   15.8                         23.4                   18.2   \n",
       "63                    2.7                          8.9                    9.5   \n",
       "64                    5.8                         14.9                    9.6   \n",
       "65                    7.6                         27.9                   17.1   \n",
       "66                    4.8                         34.4                   35.9   \n",
       "67                    3.8                         46.6                   28.0   \n",
       "68                    3.6                         29.6                   23.0   \n",
       "69                    4.0                         10.4                   11.7   \n",
       "70                    4.0                         27.6                   28.3   \n",
       "71                    0.9                          5.1                    8.0   \n",
       "72                    1.1                         16.9                   20.8   \n",
       "73                    1.0                          3.4                    8.7   \n",
       "74                    0.8                         13.2                   15.0   \n",
       "75                    3.6                         15.4                    7.1   \n",
       "76                    4.1                         18.2                    9.2   \n",
       "77                    4.7                         19.7                   12.9   \n",
       "\n",
       "    percent_25_without_diploma  percent_dependent  per_capita_income  \\\n",
       "0                         18.2               27.5              23939   \n",
       "1                         20.8               38.5              23040   \n",
       "2                         11.8               22.2              35787   \n",
       "3                         13.4               25.5              37524   \n",
       "4                          4.5               26.2              57123   \n",
       "5                          2.6               17.0              60058   \n",
       "6                          3.6               21.5              71551   \n",
       "7                          2.5               22.6              88669   \n",
       "8                          7.4               35.3              40959   \n",
       "9                         11.5               39.5              32875   \n",
       "10                        13.4               35.5              27751   \n",
       "11                         4.9               40.5              44164   \n",
       "12                        14.4               39.0              26576   \n",
       "13                        32.9               32.0              21323   \n",
       "14                        19.3               34.0              24336   \n",
       "15                        22.4               31.6              27249   \n",
       "16                        16.2               33.6              26282   \n",
       "17                        23.5               38.6              22014   \n",
       "18                        37.3               37.3              15461   \n",
       "19                        41.6               36.4              15089   \n",
       "20                        24.7               31.0              20039   \n",
       "21                        14.8               26.2              31908   \n",
       "22                        35.4               38.0              13781   \n",
       "23                        12.9               21.7              43198   \n",
       "24                        24.4               37.9              15957   \n",
       "25                        24.5               43.6              10934   \n",
       "26                        21.3               43.2              12961   \n",
       "27                         9.6               22.2              44689   \n",
       "28                        27.6               42.7              12034   \n",
       "29                        54.8               33.8              10402   \n",
       "..                         ...                ...                ...   \n",
       "48                        16.9               41.2              17949   \n",
       "49                        13.1               38.6              20588   \n",
       "50                        21.0               39.5              14685   \n",
       "51                        31.9               42.8              17104   \n",
       "52                        20.5               42.1              16563   \n",
       "53                        27.5               51.5               8201   \n",
       "54                        19.2               42.9              22677   \n",
       "55                        19.3               38.1              26353   \n",
       "56                        35.9               39.2              16134   \n",
       "57                        45.1               39.3              13089   \n",
       "58                        32.9               35.6              16954   \n",
       "59                        22.2               31.3              22694   \n",
       "60                        41.5               38.9              12765   \n",
       "61                        37.0               37.7              15754   \n",
       "62                        51.5               38.8              12171   \n",
       "63                        18.8               37.6              25113   \n",
       "64                        33.6               39.6              16907   \n",
       "65                        31.2               40.6              13231   \n",
       "66                        26.3               40.7              11317   \n",
       "67                        28.5               42.5              11888   \n",
       "68                        16.5               41.0              17285   \n",
       "69                        17.7               36.9              23482   \n",
       "70                        18.5               41.9              15528   \n",
       "71                         3.7               40.5              39523   \n",
       "72                        13.7               42.6              19713   \n",
       "73                         4.3               36.8              34381   \n",
       "74                        10.8               40.3              27149   \n",
       "75                        10.9               30.3              25828   \n",
       "76                         9.7               23.8              33385   \n",
       "77                        19.5               33.5              28202   \n",
       "\n",
       "    hardship_index  income  \n",
       "0        39.000000   False  \n",
       "1        46.000000   False  \n",
       "2        20.000000    True  \n",
       "3        17.000000    True  \n",
       "4         6.000000    True  \n",
       "5         5.000000    True  \n",
       "6         2.000000    True  \n",
       "7         1.000000    True  \n",
       "8         8.000000    True  \n",
       "9        21.000000    True  \n",
       "10       25.000000    True  \n",
       "11       11.000000    True  \n",
       "12       33.000000    True  \n",
       "13       53.000000   False  \n",
       "14       35.000000   False  \n",
       "15       34.000000    True  \n",
       "16       28.000000    True  \n",
       "17       50.000000   False  \n",
       "18       70.000000   False  \n",
       "19       71.000000   False  \n",
       "20       42.000000   False  \n",
       "21       23.000000    True  \n",
       "22       85.000000   False  \n",
       "23       10.000000    True  \n",
       "24       73.000000   False  \n",
       "25       92.000000   False  \n",
       "26       83.000000   False  \n",
       "27       15.000000    True  \n",
       "28       87.000000   False  \n",
       "29       96.000000   False  \n",
       "..             ...     ...  \n",
       "48       52.000000   False  \n",
       "49       51.000000   False  \n",
       "50       65.000000   False  \n",
       "51       64.000000   False  \n",
       "52       62.000000   False  \n",
       "53       98.000000   False  \n",
       "54       44.000000   False  \n",
       "55       32.000000    True  \n",
       "56       67.000000   False  \n",
       "57       84.000000   False  \n",
       "58       61.000000   False  \n",
       "59       43.000000   False  \n",
       "60       91.000000   False  \n",
       "61       69.000000   False  \n",
       "62       93.000000   False  \n",
       "63       29.000000   False  \n",
       "64       56.000000   False  \n",
       "65       80.000000   False  \n",
       "66       89.000000   False  \n",
       "67       94.000000   False  \n",
       "68       66.000000   False  \n",
       "69       37.000000   False  \n",
       "70       74.000000   False  \n",
       "71       12.000000    True  \n",
       "72       48.000000   False  \n",
       "73       16.000000    True  \n",
       "74       30.000000    True  \n",
       "75       24.000000    True  \n",
       "76       19.000000    True  \n",
       "77       49.506494    True  \n",
       "\n",
       "[78 rows x 8 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_classification = pickle.load(open('ex_classification_cleaned.sav', 'rb'))\n",
    "print(ex_classification.dtypes)\n",
    "ex_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    50\n",
      "True     28\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ex_classification['income'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Set the random seed to 2.\n",
    "##### Separate the predictors from the data by dropping the target variable `income` and save as `ex_X`.\n",
    "##### Save target variable `income` as an np array to `ex_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate target from data.\n",
    "ex_y = np.array(ex_classification['income'])\n",
    "# Separate predictors from data.\n",
    "ex_X = ex_classification.drop(['income'], axis = 1)\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### Split our data `ex_X` and `ex_y` into training and test sets. Split 70% into the training set and remaining 30% into the test set.\n",
    "##### Save them as `ex_X_train`, `ex_X_test`, `ex_y_train` and `ex_y_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets, use a 70 test - 30 train split.\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = train_test_split(ex_X,\n",
    "                                                    ex_y,\n",
    "                                                    test_size = .3\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Instantiate the logistic regression model and save it to `ex_logistic_regression_model`. Print it.\n",
    "##### Fit the model with our training sets `ex_X_train` and `ex_y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ex_logistic_regression_model = linear_model.LogisticRegression()\n",
    "print(ex_logistic_regression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model.\n",
    "ex_logistic_regression_model.fit(ex_X_train,ex_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "##### Now use the trained model to predict on our test set `ex_X_test`. Save as `ex_predicted_values`.\n",
    "##### Print the vector of predictions obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False  True  True False False False  True False False\n",
      " False False  True False False False False False False False  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data.\n",
    "ex_predicted_values = ex_logistic_regression_model.predict(ex_X_test)\n",
    "print(ex_predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Create a confusion matrix using the `metrics.confusion_matrix()` function.\n",
    "##### Save as `ex_conf_matrix` and print.\n",
    "##### What can you determine by looking at the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0]\n",
      " [ 0  7]]\n",
      "Accuracy on test data:  1.0\n",
      "it accuractly predicted are acurate possitive and negative values\n"
     ]
    }
   ],
   "source": [
    "# Take a look at test data confusion matrix.\n",
    "ex_conf_matrix_test = metrics.confusion_matrix(ex_y_test,\n",
    "                                               ex_predicted_values)\n",
    "print(ex_conf_matrix_test)\n",
    "\n",
    "# Compute test model accuracy score.\n",
    "ex_test_accuracy_score = metrics.accuracy_score(ex_y_test, \n",
    "                                                ex_predicted_values)\n",
    "print(\"Accuracy on test data: \", ex_test_accuracy_score)\n",
    "print('it accuractly predicted are acurate possitive and negative values')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Calculate the accuracy of our model by comparing our predicted values against our test set `ex_y_test`.\n",
    "##### Save as `ex_test_accuracy` and print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        poor       1.00      1.00      1.00        17\n",
      "        rich       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of target names to interpret class assignments.\n",
    "ex_target_names = ['poor', 'rich']\n",
    "# Print an entire classification report.\n",
    "ex_class_report = metrics.classification_report(\n",
    "    ex_y_test,\n",
    "    ex_predicted_values,\n",
    "    target_names = ex_target_names\n",
    ")\n",
    "print(ex_class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    metrics  values             model\n",
      "0  accuracy  0.8333             knn_5\n",
      "1  accuracy  0.9359  knn_GridSearchCV\n",
      "2  accuracy  0.9583            knn_27\n",
      "3  accuracy  1.0000          logistic\n"
     ]
    }
   ],
   "source": [
    "ex_model_final = pickle.load(open(\"ex_model_final.sav\",\"rb\"))\n",
    "ex_model_final = ex_model_final.append({'metrics' : \"accuracy\" ,\n",
    "'values' : round(ex_test_accuracy_score,4),\n",
    "'model':'logistic' } ,\n",
    "ignore_index = True)\n",
    "print(ex_model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Load the pickled dataset `ex_model_final.sav` which we saved earlier.\n",
    "##### Add the above accuracy score `ex_test_accuracy` calculated above to the dataframe `ex_model_final` which we created earlier and view the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see above\n"
     ]
    }
   ],
   "source": [
    "print('see above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### Create a list named `target_names` with the class names, `Low Income` and `High Income`.\n",
    "##### Create a report `ex_class_report` using our test values `ex_y_test` and predicted values `ex_predicted_values` with the following columns:\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- target_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Get the probabilities of predicted values `ex_X_test` and save as `ex_test_probabilities`.\n",
    "##### Then, calculate the probabilities of test predictions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 5.50830938e-17]\n",
      " [1.00000000e+00 1.54851521e-14]\n",
      " [2.22044605e-16 1.00000000e+00]\n",
      " [9.99999516e-01 4.84215062e-07]\n",
      " [1.68305934e-03 9.98316941e-01]\n",
      " [2.70894418e-12 1.00000000e+00]\n",
      " [1.00000000e+00 7.22182142e-17]\n",
      " [1.00000000e+00 1.05269881e-11]\n",
      " [9.96225633e-01 3.77436656e-03]\n",
      " [1.11022302e-15 1.00000000e+00]\n",
      " [7.33179305e-01 2.66820695e-01]\n",
      " [9.99999128e-01 8.72050560e-07]\n",
      " [9.99793525e-01 2.06474641e-04]\n",
      " [1.00000000e+00 5.93667446e-15]\n",
      " [4.54257056e-07 9.99999546e-01]\n",
      " [1.00000000e+00 4.11708558e-23]\n",
      " [9.99999995e-01 4.72369240e-09]\n",
      " [9.99999626e-01 3.73843806e-07]\n",
      " [9.99999999e-01 5.86595716e-10]\n",
      " [9.99967823e-01 3.21765466e-05]\n",
      " [5.84655210e-01 4.15344790e-01]\n",
      " [1.00000000e+00 2.86087846e-10]\n",
      " [8.58011613e-04 9.99141988e-01]\n",
      " [4.38774880e-01 5.61225120e-01]]\n",
      "[5.50830938e-17 1.54851521e-14 1.00000000e+00 4.84215062e-07\n",
      " 9.98316941e-01 1.00000000e+00 7.22182142e-17 1.05269881e-11\n",
      " 3.77436656e-03 1.00000000e+00 2.66820695e-01 8.72050560e-07\n",
      " 2.06474641e-04 5.93667446e-15 9.99999546e-01 4.11708558e-23\n",
      " 4.72369240e-09 3.73843806e-07 5.86595716e-10 3.21765466e-05\n",
      " 4.15344790e-01 2.86087846e-10 9.99141988e-01 5.61225120e-01]\n"
     ]
    }
   ],
   "source": [
    "# Get probabilities instead of predicted values.\n",
    "ex_test_probabilities = ex_logistic_regression_model.predict_proba(ex_X_test)\n",
    "print(ex_test_probabilities[:, :])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_test_predictions = ex_test_probabilities[:, 1]\n",
    "print(ex_test_predictions[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "##### Derive the `fpr`, `tpr`, and the `threshold` using our test set and predictions.\n",
    "##### Then, calculate the `auc` using the derived `fpr` and `tpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive:  [0. 0. 0. 1.]\n",
      "True positive:  [0.         0.14285714 1.         1.        ]\n",
      "Threshold:  [2.00000000e+00 1.00000000e+00 5.61225120e-01 4.11708558e-23]\n"
     ]
    }
   ],
   "source": [
    "# Get FPR, TPR, and threshold values.\n",
    "fpr, tpr, threshold = metrics.roc_curve(ex_y_test,            #<- test data labels\n",
    "                                        ex_test_predictions)  #<- predicted probabilities\n",
    "print(\"False positive: \", fpr[:5])\n",
    "print(\"True positive: \", tpr[:5])\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "##### Plot and ROC curve plot using the values derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Get AUC by providing the FPR and TPR.\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, \n",
    "         tpr, \n",
    "         'b', \n",
    "         label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "##### For the purpose of demonstrating how to deal with categorical variables, we will convert `hardship_index` to a categorical variable with three variables.\n",
    "##### Take a look at the summary of `hardship_index` using .describe().\n",
    "##### To convert into categorical variable, for those rows where `hardship_index` is lower than the 25, convert to `25 or below`, for those higher than 25 but lower than 73, convert to `Between 25 and 73`, and the rest of the rows to `Higher than 73`.\n",
    "##### Print the value counts of the new `hardship_index` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    78.000000\n",
       "mean     49.506494\n",
       "std      28.503645\n",
       "min       1.000000\n",
       "25%      25.250000\n",
       "50%      49.753247\n",
       "75%      73.750000\n",
       "max      98.000000\n",
       "Name: hardship_index, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_classification['hardship_index'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_classification['hardship_index'] = np.where(ex_classification['hardship_index'] <= 25, \"25 or Below\",\n",
    "                                     np.where(ex_classification['hardship_index'] < 73, 'Between 25 and 73', '73 and above'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Between 25 and 73    37\n",
       "73 and above         21\n",
       "25 or Below          20\n",
       "Name: hardship_index, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_classification.hardship_index.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "##### Transform `hardship_index` column into a dummy variable and save it into a dataframe named `hardship_dummy`.\n",
    "##### Make sure to set `drop_first` as `True`.\n",
    "##### Drop the original `hardship_index` variable from `ex_classification` and concatenate `hardship_dummy` into `ex_classification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>73 and above</th>\n",
       "      <th>Between 25 and 73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   73 and above  Between 25 and 73\n",
       "0             0                  1\n",
       "1             0                  1\n",
       "2             0                  0\n",
       "3             0                  0\n",
       "4             0                  0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert into dummy variables.\n",
    "hardhsip_dummy = pd.get_dummies(ex_classification['hardship_index'], drop_first = True)\n",
    "hardhsip_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Between 25 and 73\n",
       "1    Between 25 and 73\n",
       "2          25 or Below\n",
       "3          25 or Below\n",
       "4          25 or Below\n",
       "Name: hardship_index, dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_classification['hardship_index'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_house_crowded</th>\n",
       "      <th>percent_house_below_poverty</th>\n",
       "      <th>percent_16_unemployed</th>\n",
       "      <th>percent_25_without_diploma</th>\n",
       "      <th>percent_dependent</th>\n",
       "      <th>per_capita_income</th>\n",
       "      <th>income</th>\n",
       "      <th>73 and above</th>\n",
       "      <th>Between 25 and 73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>18.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>23939</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>17.2</td>\n",
       "      <td>8.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>38.5</td>\n",
       "      <td>23040</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>11.8</td>\n",
       "      <td>22.2</td>\n",
       "      <td>35787</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4</td>\n",
       "      <td>10.9</td>\n",
       "      <td>8.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>25.5</td>\n",
       "      <td>37524</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>26.2</td>\n",
       "      <td>57123</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_house_crowded  percent_house_below_poverty  percent_16_unemployed  \\\n",
       "0                    7.7                         23.6                    8.7   \n",
       "1                    7.8                         17.2                    8.8   \n",
       "2                    3.8                         24.0                    8.9   \n",
       "3                    3.4                         10.9                    8.2   \n",
       "4                    0.3                          7.5                    5.2   \n",
       "\n",
       "   percent_25_without_diploma  percent_dependent  per_capita_income  income  \\\n",
       "0                        18.2               27.5              23939   False   \n",
       "1                        20.8               38.5              23040   False   \n",
       "2                        11.8               22.2              35787    True   \n",
       "3                        13.4               25.5              37524    True   \n",
       "4                         4.5               26.2              57123    True   \n",
       "\n",
       "   73 and above  Between 25 and 73  \n",
       "0             0                  1  \n",
       "1             0                  1  \n",
       "2             0                  0  \n",
       "3             0                  0  \n",
       "4             0                  0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop `age` from the data.\n",
    "ex_classification.drop(['hardship_index'], axis = 1, inplace = True)\n",
    "# Concatenate `age_dummy` to our dataset.\n",
    "ex_classification2 = pd.concat([ex_classification,hardhsip_dummy],axis=1)\n",
    "ex_classification2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Pickle `ex_classification` as `ex_classification_dummies.sav` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ex_classification2, open(\"ex_classification_dummies.sav\",\"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### As we did before, split the dataset into predictors `ex_X` and target `ex_y`.\n",
    "##### Save target variable `income` as an np array to `ex_y`.\n",
    "##### Save the rest of the variables to `ex_X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from data.\n",
    "ex_y2 = np.array(ex_classification2['income'])\n",
    "# Separate predictors from data.\n",
    "ex_X2 = ex_classification2.drop(['income'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Split our data `ex_X` and `ex_y` into training and test sets. Split 70% into the training set and remaining 30% into the test set.\n",
    "##### Save them as `ex_X_train`, `ex_X_test`, `ex_y_train` and `ex_y_test` respectively.\n",
    "##### Make sure to set the seed to 2 so you can replicate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets, use a 70 test - 30 train split.\n",
    "ex_X_train2, ex_X_test2, ex_y_train2, ex_y_test2 = train_test_split(ex_X2,\n",
    "                                                    ex_y2,\n",
    "                                                    test_size = .3\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "##### Instantiate the logistic regression model and save it to `ex_logistic_regression_model`. Print it.\n",
    "##### Fit the model with our training sets `ex_X_train` and `ex_y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "ex_logistic_regression_model2 = linear_model.LogisticRegression()\n",
    "print(ex_logistic_regression_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model.\n",
    "ex_logistic_regression_model2.fit(ex_X_train2,ex_y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "##### Now use the trained model to predict on our test set `ex_X_test`. Save as `ex_predicted_values`.\n",
    "##### Print the vector of predictions obtained.\n",
    "##### Create a confusion matrix using the `metrics.confusion_matrix()` function.\n",
    "##### Save as `ex_conf_matrix` and print.\n",
    "##### Calculate the accuracy of our model by comparing our predicted values against our test set `ex_y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False  True  True False False False  True False False\n",
      " False False  True False False False False False False False  True  True]\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data.\n",
    "ex_predicted_values2 = ex_logistic_regression_model2.predict(ex_X_test2)\n",
    "print(ex_predicted_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0]\n",
      " [ 0  7]]\n",
      "Accuracy on test data:  1.0\n",
      "it accuractly predicted are acurate possitive and negative values\n"
     ]
    }
   ],
   "source": [
    "# Take a look at test data confusion matrix.\n",
    "ex_conf_matrix_test2 = metrics.confusion_matrix(ex_y_test2,\n",
    "                                               ex_predicted_values2)\n",
    "print(ex_conf_matrix_test2)\n",
    "\n",
    "# Compute test model accuracy score.\n",
    "ex_test_accuracy_score2 = metrics.accuracy_score(ex_y_test2, \n",
    "                                                ex_predicted_values2)\n",
    "print(\"Accuracy on test data: \", ex_test_accuracy_score2)\n",
    "print('it accuractly predicted are acurate possitive and negative values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "##### Add the accuracy score `ex_test_accuracy` calculated above with the model name as `logistic_withdummies` to the dataframe `ex_model_final` which we created earlier and view the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    metrics  values                 model\n",
      "0  accuracy  0.8333                 knn_5\n",
      "1  accuracy  0.9359      knn_GridSearchCV\n",
      "2  accuracy  0.9583                knn_27\n",
      "3  accuracy  1.0000              logistic\n",
      "4  accuracy  1.0000  logistic_withdummies\n"
     ]
    }
   ],
   "source": [
    "ex_model_final = ex_model_final.append({'metrics' : \"accuracy\" ,\n",
    "                                        'values' : round(ex_test_accuracy_score2,4),\n",
    "                                        'model':'logistic_withdummies' } ,\n",
    "                                       ignore_index = True)\n",
    "print(ex_model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "##### Create a list named `target_names` with the class names as `Low Income` and `High Income`.\n",
    "##### Create a report `ex_class_report` using our test values `ex_y_test` and predicted values `ex_predicted_values` with the following columns:\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Poor SOB       1.00      1.00      1.00        17\n",
      "    Rich SOB       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of target names to interpret class assignments.\n",
    "ex_target_names2 = ['Poor SOB', 'Rich SOB']\n",
    "# Print an entire classification report.\n",
    "ex_class_report2 = metrics.classification_report(\n",
    "    ex_y_test2,\n",
    "    ex_predicted_values2,\n",
    "    target_names = ex_target_names2\n",
    ")\n",
    "print(ex_class_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "##### Get the probabilities of predicted values `ex_X_test` and save as `ex_test_probabilities`.\n",
    "##### Then calculate the probabilities of test predictions named `ex_test_predictions`  only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.89363387e-10]\n",
      " [9.99999991e-01 9.23609528e-09]\n",
      " [2.76535639e-09 9.99999997e-01]\n",
      " [9.99753716e-01 2.46283556e-04]\n",
      " [1.86915590e-02 9.81308441e-01]\n",
      " [9.66260165e-08 9.99999903e-01]\n",
      " [1.00000000e+00 1.45253358e-10]\n",
      " [9.99999711e-01 2.89491248e-07]\n",
      " [9.71318528e-01 2.86814722e-02]\n",
      " [4.14686308e-09 9.99999996e-01]\n",
      " [6.11915205e-01 3.88084795e-01]\n",
      " [9.99904769e-01 9.52313685e-05]\n",
      " [9.86717616e-01 1.32823845e-02]\n",
      " [9.99999999e-01 7.53974422e-10]\n",
      " [2.77184521e-04 9.99722815e-01]\n",
      " [1.00000000e+00 2.20930904e-14]\n",
      " [9.99997215e-01 2.78538746e-06]\n",
      " [9.99848945e-01 1.51055324e-04]\n",
      " [9.99996335e-01 3.66528252e-06]\n",
      " [9.93855949e-01 6.14405096e-03]\n",
      " [6.79961113e-01 3.20038887e-01]\n",
      " [9.99996474e-01 3.52611935e-06]\n",
      " [1.29695222e-02 9.87030478e-01]\n",
      " [3.26756869e-01 6.73243131e-01]]\n",
      "[1.89363387e-10 9.23609528e-09 9.99999997e-01 2.46283556e-04\n",
      " 9.81308441e-01 9.99999903e-01 1.45253358e-10 2.89491248e-07\n",
      " 2.86814722e-02 9.99999996e-01 3.88084795e-01 9.52313685e-05\n",
      " 1.32823845e-02 7.53974422e-10 9.99722815e-01 2.20930904e-14\n",
      " 2.78538746e-06 1.51055324e-04 3.66528252e-06 6.14405096e-03\n",
      " 3.20038887e-01 3.52611935e-06 9.87030478e-01 6.73243131e-01]\n"
     ]
    }
   ],
   "source": [
    "# Get probabilities instead of predicted values.\n",
    "ex_test_probabilities2 = ex_logistic_regression_model2.predict_proba(ex_X_test2)\n",
    "print(ex_test_probabilities2[:, :])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_test_predictions2 = ex_test_probabilities2[:, 1]\n",
    "print(ex_test_predictions2[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11\n",
    "##### Derive the `fpr`, `tpr`, and the `threshold` using our test set and predictions.\n",
    "##### Then calculate the `auc` using the derived `fpr` and `tpr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive:  [0. 0. 0. 1.]\n",
      "True positive:  [0.         0.14285714 1.         1.        ]\n",
      "Threshold:  [2.00000000e+00 9.99999997e-01 6.73243131e-01 2.20930904e-14]\n"
     ]
    }
   ],
   "source": [
    "# Get FPR, TPR, and threshold values.\n",
    "fpr2, tpr2, threshold2 = metrics.roc_curve(ex_y_test2,            #<- test data labels\n",
    "                                        ex_test_predictions2)  #<- predicted probabilities\n",
    "print(\"False positive: \", fpr2[:5])\n",
    "print(\"True positive: \", tpr2[:5])\n",
    "print(\"Threshold: \", threshold2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Get AUC by providing the FPR and TPR.\n",
    "auc2 = metrics.auc(fpr2, tpr2)\n",
    "print(\"Area under the ROC curve: \", auc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "##### Plot and ROC curve plot using the values derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, \n",
    "         tpr, \n",
    "         'b', \n",
    "         label = 'AUC = %0.2f' % auc,\n",
    "         alpha = .2)\n",
    "plt.plot(fpr2, \n",
    "         tpr2, \n",
    "         'r', \n",
    "         label = 'AUC2 = %0.2f' % auc2,\n",
    "         alpha = .2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### Create regularization penalty space named `penalty` and regularization constant space `C` as in slides.\n",
    "##### Then create the hyperparameter dictionary named `hyperparameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization constant:  [1.00000000e+00 1.29154967e+01 1.66810054e+02 2.15443469e+03\n",
      " 2.78255940e+04 3.59381366e+05 4.64158883e+06 5.99484250e+07\n",
      " 7.74263683e+08 1.00000000e+10]\n",
      "{'C': array([1.00000000e+00, 1.29154967e+01, 1.66810054e+02, 2.15443469e+03,\n",
      "       2.78255940e+04, 3.59381366e+05, 4.64158883e+06, 5.99484250e+07,\n",
      "       7.74263683e+08, 1.00000000e+10]), 'penalty': ['l1', 'l2']}\n"
     ]
    }
   ],
   "source": [
    "# Create regularization penalty space.\n",
    "ex_penalty = ['l1', 'l2']\n",
    "# Create regularization constant space.\n",
    "ex_C = np.logspace(0, 10, 10)\n",
    "print(\"Regularization constant: \", ex_C)\n",
    "# Create hyperparameter options dictionary.\n",
    "ex_hyperparameters = dict(C = ex_C, penalty = ex_penalty)\n",
    "print(ex_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### Set up the 5-fold cross-validation function named `ex_clf`  with the parameters created above.\n",
    "##### Fit the function with our training set. Save as `ex_best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': array([1.00000e+00, 1.29155e+01, 1.66810e+02, 2.15443e+03, 2.78256e+04,\n",
       "       3.59381e+05, 4.64159e+06, 5.99484e+07, 7.74264e+08, 1.00000e+10]), 'penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search 10-fold cross-validation with above parameters.\n",
    "ex_clf = GridSearchCV(\n",
    "    linear_model.LogisticRegression(), #<- function to optimize\n",
    "    ex_hyperparameters,                   #<- grid search parameters\n",
    "    cv = 10,                           #<- 10-fold cv\n",
    "    verbose = 0)                       #<- no messages to show\n",
    "# Fit CV grid search.\n",
    "ex_best_model = ex_clf.fit(ex_X_train2, ex_y_train2)\n",
    "ex_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### Derive the best penalty and constant parameters. Print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best penalty:  l1\n",
      "Best C:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Get best penalty and constant parameters.\n",
    "ex_penalty = ex_best_model.best_estimator_.get_params()['penalty']\n",
    "ex_constant = ex_best_model.best_estimator_.get_params()['C']\n",
    "print('Best penalty: ', ex_penalty)\n",
    "print('Best C: ', ex_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "##### Print the vector of predicted values named `ex_best_predicted_values`.\n",
    "##### Also find the accuracy score `ex_best_accuracy_score` and print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False  True  True False False False  True False False\n",
      " False False  True False False False False False False False  True  True]\n",
      "Accuracy on test data (best model):  1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data using best model.\n",
    "ex_best_predicted_values = ex_best_model.predict(ex_X_test2)\n",
    "print(ex_best_predicted_values)\n",
    "# Compute best model accuracy score.\n",
    "ex_best_accuracy_score = metrics.accuracy_score(ex_y_test2, ex_best_predicted_values)\n",
    "print(\"Accuracy on test data (best model): \", ex_best_accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "##### Compute the confusion matrix and classification report as above. Save as `ex_best_confusion_matrix` and `ex_best_class_report` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  0]\n",
      " [ 0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Low value       1.00      1.00      1.00        17\n",
      "  High value       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix for best model.\n",
    "ex_best_confusion_matrix = metrics.confusion_matrix(ex_y_test2, ex_best_predicted_values)\n",
    "print(ex_best_confusion_matrix)\n",
    "# Create a list of target names to interpret class assignments.\n",
    "target_names = ['Low value', 'High value']\n",
    "# Compute classification report for best model.\n",
    "ex_best_class_report = metrics.classification_report(ex_y_test2, ex_best_predicted_values,\n",
    "target_names = target_names)\n",
    "print(ex_best_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "##### Add this accuracy score with the model name as `logistic_tuned` to our `ex_model_final` dataframe.\n",
    "##### Pickle this dataset as `ex_model_log_penalty.sav`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    metrics  values                 model\n",
      "0  accuracy  0.8333                 knn_5\n",
      "1  accuracy  0.9359      knn_GridSearchCV\n",
      "2  accuracy  0.9583                knn_27\n",
      "3  accuracy  1.0000              logistic\n",
      "4  accuracy  1.0000  logistic_withdummies\n",
      "5  accuracy  1.0000        logistic_tuned\n"
     ]
    }
   ],
   "source": [
    "ex_model_final = ex_model_final.append({'metrics' : \"accuracy\" ,\n",
    "                                        'values' : round(ex_best_accuracy_score,4),\n",
    "                                        'model':'logistic_tuned' } ,\n",
    "                                       ignore_index = True)\n",
    "print(ex_model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ex_model_final, open('ex_model_log_penalty.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "##### Same as before, compute the test probabilities `ex_best_test_probabilities` and \n",
    "##### test predictions `ex_best_test_predictions`.\n",
    "##### Derive the metrics `fpr`, `tpr`, and `threshold` to plot the ROC curve. Also derive the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.44381691e-13]\n",
      " [1.00000000e+00 1.25780264e-11]\n",
      " [1.22835075e-12 1.00000000e+00]\n",
      " [9.99991225e-01 8.77509176e-06]\n",
      " [5.19467086e-03 9.94805329e-01]]\n",
      "[1.44381691e-13 1.25780264e-11 1.00000000e+00 8.77509176e-06\n",
      " 9.94805329e-01]\n"
     ]
    }
   ],
   "source": [
    "# Get probabilities instead of predicted values.\n",
    "ex_best_test_probabilities = ex_best_model.predict_proba(ex_X_test2)\n",
    "print(ex_best_test_probabilities[0:5, ])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_best_test_predictions = ex_best_test_probabilities[:, 1]\n",
    "print(ex_best_test_predictions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Get ROC curve metrics.\n",
    "best_fpr, best_tpr, best_threshold = metrics.roc_curve(ex_y_test2, ex_best_test_predictions)\n",
    "best_auc = metrics.auc(best_fpr, best_tpr)\n",
    "print(best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "##### Plot the ROC curve for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, \n",
    "         tpr, \n",
    "         'm', \n",
    "         label = 'AUC = %0.2f' % auc,\n",
    "         alpha = .2)\n",
    "plt.plot(best_fpr, \n",
    "         best_tpr, \n",
    "         'r', \n",
    "         label = 'Best_AUC = %0.2f' % best_auc,\n",
    "         alpha = .2)\n",
    "plt.plot(fpr, \n",
    "         tpr, \n",
    "         'b', \n",
    "         label = 'AUC = %0.2f' % auc,\n",
    "         alpha = .2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
